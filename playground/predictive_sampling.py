from typing import Tuple

import flax.linen as nn
import jax
import jax.numpy as jnp
from brax.envs.base import PipelineEnv, State
from brax.training.types import Params
from flax import struct


@struct.dataclass
class PredictiveSamplingOptions:
    """Hyperparameters for predictive sampling policy search.

    episode_length: The number of timesteps in each episode.
    planning_horizon: The number of timesteps to plan ahead.
    num_envs: The number of parallel environments to use.
    num_samples: The number of samples to take in each environment.
    noise_std: The standard deviation of the noise added to actions.
    """

    episode_length: int
    planning_horizon: int
    num_envs: int
    num_samples: int
    noise_std: float


class PredictiveSampling:
    """Policy learning based on predictive sampling.

    The basic idea is to learn a trajectory-optimizing policy

        u₀, u₁, ... = π(y₀; θ)

    by regressing on training data generated by predictive sampling
    (Howell et al., https://arxiv.org/abs/2212.00541).
    """

    def __init__(
        self,
        env: PipelineEnv,
        policy: nn.Module,
        options: PredictiveSamplingOptions,
        seed: int = 0,
    ):
        """Initialize the predictive sampling policy search algorithm.

        Args:
            env: The environment to train on.
            policy: A network module mapping observations to an action sequence.
            options: The hyperparameters for the algorithm.
            seed: The random seed to use for parameter initialization.
        """
        self.env = env
        self.policy = policy
        self.options = options
        self.seed = seed

        # TODO: check the policy has the correct output size

    def rollout(
        self, start_state: State, action_sequence: jnp.ndarray
    ) -> jnp.ndarray:
        """Apply the given action sequence and return the total reward.

        Args:
            start_state: The initial state of the environment.
            action_sequence: A sequence of actions to execute.

        Returns:
            The total reward from the rollout.
        """
        raise NotImplementedError

    def choose_action_sequence(
        self,
        start_state: State,
        last_action_sequence: jnp.ndarray,
        policy_params: Params,
        rng: jax.random.PRNGKey,
    ) -> jnp.ndarray:
        """Use predictive sampling to get a reasonable action sequence.

        Half of the samples are from a normal distribution around the last
        action sequence, while the other half are from a normal distribution
        around the policy output.

        Args:
            start_state: The initial state of the environment.
            last_action_sequence: The last action sequence executed.
            policy_params: The parameters of the policy.
            rng: The random key to use.

        Returns:
            The sampled action sequence with the highest reward.
        """
        # Sample around the last action sequence
        # Sample around the policy output
        # Roll out each action sequence and return the best one
        raise NotImplementedError

    def episode(
        self,
        policy_params: Params,
        rng: jax.random.PRNGKey,
    ) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """Collect an episode of training data from a random initial state.

        Args:
            policy_params: The curren policy parameters
            rng: The random key to use.

        Returns:
            A dataset of (start_state, action_sequence) pairs.
        """
        raise NotImplementedError
